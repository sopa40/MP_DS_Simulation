{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MP DS SS22 - Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "from typing import Tuple, List\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from joblib import dump, load\n",
    "import shutil\n",
    "from string import Template\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "indexCol = \"name\"\n",
    "numericCols = ['%{0}'.format(\"cpu\"), 'rchar', 'wchar', \"instructions\",\"peak_vmem\"]\n",
    "relevantColumns = numericCols + [indexCol, \"name\"]\n",
    "inputPath = \"./traces\"\n",
    "modelDir = \"./models\"\n",
    "plotDir = \"./plots\"\n",
    "templatesDir = \"./templates\"\n",
    "preRenderedDir = \"./preRenderedTemplates\"\n",
    "renderedDir = \"./rendered\"\n",
    "executionName = \"execution_count\"\n",
    "conversionDict = {\n",
    "    \"B\": 1,\n",
    "    \"KB\": 1024,\n",
    "    \"MB\": 1024**2,\n",
    "    \"GB\": 1024**3,\n",
    "    \"TB\": 1024**4,\n",
    "    \"PB\": 1024**5,\n",
    "    \"EC\": 1024**6\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pandas data frame from the contents of the generated trace. <-- Change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFile(filename: str, filePath: str) -> Tuple[str,pd.DataFrame]:\n",
    "    return tuple([filename,pd.read_csv(f\"{filePath}/{filename}\", sep=\"\\t\").astype(str)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted = [int(f.replace(\".txt\", \"\")) for f in os.listdir(inputPath) if not f.startswith('.')]\n",
    "sorted.sort()\n",
    "traceTuples = [parseFile(f\"{str(f)}.txt\",inputPath) for f in sorted]\n",
    "print(traceTuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table for each task with the relevant values in it. \n",
    "\n",
    "Additionally create a list of the file names where the index responds to the to the row index in the previously created table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searches for occurance of '(' and removes everything including and after that char.\n",
    "def cleanName(name: str):\n",
    "    nameIndex = name.find('(')\n",
    "    if nameIndex > 0:\n",
    "        return name[:nameIndex].rstrip()\n",
    "    else:\n",
    "\t    return name\n",
    "    \n",
    "# returns concated values consisting of the taskIDs and the cleaned names\n",
    "def getTasks(data: pd.DataFrame):\n",
    "    return data[indexCol].apply(cleanName)\n",
    "\n",
    "# converts a single value using the conversion dict\n",
    "def convert(item: str):\n",
    "    item = re.match('(\\d+[.,]\\d{1,2}|\\d+)', item).group()\n",
    "    for key in conversionDict.keys():\n",
    "        if key in item:\n",
    "            item = float(item) * conversionDict[key]\n",
    "            break\n",
    "    return item\n",
    "\n",
    "# Applies convert function to every item in Series\n",
    "def convertValues(cols: pd.Series):\n",
    "    return cols.apply(convert)\n",
    "\n",
    "# Calculate the mean for multiple executions of a task\n",
    "def calcMean(col: pd.Series):\n",
    "    return col.str.extract('(\\d+[.,]\\d{1,2}|\\d+)', expand=False).astype(float).mean().round(decimals=2)\n",
    "\n",
    "# Creates dictionaries that contain tuples which in turn contain the tasks and the values of the different traces.\n",
    "def createDictionaries(tasks: pd.Series, traces: Tuple):\n",
    "    \n",
    "    dfList = {}\n",
    "    rowLookup = {}\n",
    "    rowLookupCount = 0\n",
    "    for task in tasks:\n",
    "        dfList[task] = pd.DataFrame(columns=numericCols + [executionName])\n",
    "        for fileName,trace in traces:\n",
    "            trace[indexCol] = trace[indexCol].apply(cleanName)\n",
    "            \n",
    "            # Create dict to lookup which row came from which trace.\n",
    "            if fileName not in rowLookup.keys():\n",
    "                rowLookup[fileName] = rowLookupCount\n",
    "                rowLookupCount += 1\n",
    "                \n",
    "            # Filter the trace and convert numeric values when needed. \n",
    "            relevantTrace = trace.loc[trace[indexCol] == task][numericCols]\n",
    "            convertedValues = relevantTrace[numericCols].apply(convertValues).apply(calcMean).append(pd.Series(data=len(relevantTrace), index=[executionName]))\n",
    "            \n",
    "            # Add to end of dict.\n",
    "            dfList[task].loc[len(dfList[task])] = convertedValues.to_numpy().flatten().tolist()\n",
    "    return dfList, rowLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get taskIDs and then create taskDict + rwoLookup to later create regression model and plot the values\n",
    "tasks = list(getTasks(traceTuples[0][1]).drop_duplicates())\n",
    "taskDict, rowLookup = createDictionaries(tasks, traceTuples)\n",
    "print(taskDict)\n",
    "print(rowLookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells will create the scatter plots from the dictionary. The rowLookup is used to store which value comes from which trace. The name of the trace should also be the input size.\n",
    "\n",
    "For each task #numericCols plots will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanFileName(fileName):\n",
    "    return fileName.replace(\".txt\",\"\")\n",
    "\n",
    "# Generates a single plot and create a poly reg model based on a task\n",
    "def plotAndModel(x, y, xTitle, yTitle, title):\n",
    "    \n",
    "    plt.title(title, loc='center')\n",
    "    plt.scatter(x, y, c=\"r\", alpha=0.5)\n",
    "    plt.xlabel(xTitle)\n",
    "    plt.ylabel(yTitle)\n",
    "    \n",
    "    # fit polynomial curve\n",
    "    polyRegModel = createPolyModel(x,y)\n",
    "    regCurve = np.linspace(min(x), max(x), 100).reshape(-1, 1)\n",
    "    plt.plot(regCurve, polyRegModel.predict(regCurve))\n",
    "    \n",
    "    # Save model\n",
    "    dir = f\"{modelDir}/{title}\"\n",
    "    ensureDir(dir)\n",
    "    joblib_file = f\"{dir}/{yTitle}.pkl\"  \n",
    "    dump(polyRegModel, joblib_file)\n",
    "    \n",
    "    # Save plot\n",
    "    dir = f\"{plotDir}/{title}\"\n",
    "    ensureDir(dir)\n",
    "    plt.savefig(f\"{dir}/{yTitle}.jpg\",bbox_inches='tight', dpi=150)\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "# Attempts to create a dir.\n",
    "def ensureDir(dir: str):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    assert os.path.exists(dir), f\"{dir} existence could not be ensured.\"\n",
    "\n",
    "# Ensure that directory exists and delete all files\n",
    "def clearDir(dir: str):\n",
    "    if os.path.exists(dir):\n",
    "        shutil.rmtree(dir)\n",
    "        ensureDir(dir)\n",
    "    else:\n",
    "        ensureDir(dir)\n",
    "\n",
    "# creates and returns a polynomial regression model. Additionally \n",
    "def createPolyModel(x, y):\n",
    "    degree = 5\n",
    "    polyreg = make_pipeline(PolynomialFeatures(degree), preprocessing.StandardScaler(), LinearRegression())\n",
    "    polyreg.fit(x.reshape(-1, 1),y)\n",
    "    return polyreg\n",
    "\n",
    "# Generates all the plots for a single task\n",
    "def generateTaskPlots(traceDictionary: pd.DataFrame, title: str, rowLookup: List):\n",
    "    for col in traceDictionary:\n",
    "        plotAndModel( np.asarray(rowLookup), traceDictionary[col].to_numpy(),\"input size in KB\", col, title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowLookup = [float(cleanFileName(row)) for row in rowLookup]\n",
    "print(rowLookup)\n",
    "for dir in [modelDir, plotDir]:\n",
    "     clearDir(dir)\n",
    "for task in taskDict.keys():\n",
    "\tgenerateTaskPlots(taskDict[task].astype(float), task, rowLookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From here on the code should be split. The following code will assume the existence of previously trained models and simply generate a trace based on those models and a supplied template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following section creates a dictionary with the predicted values form each model for a given input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the models to create the trace for a single task\n",
    "def createTemplateDictionary( inputSize: float, modelName: str):\n",
    "    dir = f\"{modelDir}/{modelName}\"\n",
    "    models = [(f.replace(\".pkl\", \"\"), load(f\"{dir}/{f}\")) for f in os.listdir(dir)]\n",
    "    \n",
    "    # loop through cols and create the predicted value for each task. Then add to dictionary for rendering\n",
    "    for col, model in models:\n",
    "        templateDictionary[f\"{modelName}_{col}\"] = \"%.2f\" % model.predict(np.asarray(inputSize).reshape(-1, 1))[0]\n",
    "        templateDictionary[f\"{modelName}_name\"] = modelName\n",
    "    return templateDictionary\n",
    "     \n",
    "def plotModelAndPrediction( model, xTitle, yTitle, title):\n",
    "    \n",
    "    plt.title(title, loc='center')\n",
    "    plt.xlabel(xTitle)\n",
    "    plt.ylabel(yTitle)\n",
    "    \n",
    "    regCurve = np.linspace(0, 20000, 100).reshape(-1, 1)\n",
    "    plt.plot(regCurve, model.predict(regCurve))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for all models that can later be used to fill in the template\n",
    "inputSize = 20000\n",
    "templateDictionary = {}\n",
    "for f in os.listdir(modelDir):\n",
    "    templateDictionary = {**templateDictionary,**createTemplateDictionary(inputSize,f)}\n",
    "print(templateDictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The previously created dictionary is used to insert the predicted values into a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensureDir(templatesDir)\n",
    "ensureDir(preRenderedDir)\n",
    "ensureDir(renderedDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a template for a workflow the tasks are used to genereate a pre-rendered template that will then be rendered again to add the actual values. Pre-rendering saves time on creating the final template especially for large workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(templatesDir):\n",
    "\tif not f.startswith('.'):\n",
    "\t\twith open(f\"{templatesDir}/{f}\", \"r\") as ft:\n",
    "\t\t\ttemplate = Template(ft.read())\n",
    "\t\t\trenderedTemplate = []\n",
    "\t\t\tfor task in tasks:\n",
    "\t\t\t\trenderedTemplate.append(template.substitute({\"task\" : task}))\n",
    "\t\twith open(f\"{preRenderedDir}/{f}\", \"w\") as fr:\n",
    "\t\t\tdelimiter = \",\"\n",
    "\t\t\trenderedTemplate = f\"[{delimiter.join(renderedTemplate)}]\"\n",
    "\t\t\tfr.write(renderedTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the normal template class does not allow the identifier to begin with a numeric character it was necessary to change the idpattern.\n",
    "# The identifier now has to begin with a numeric character.\n",
    "class TraceTemplate(Template):\n",
    "    idpattern = r'(?-i:[_a-zA-Z0-9]*)'\n",
    "    \n",
    "# go through the preRenderedDir folder, render the templates and then svae them to the rendered folder\n",
    "# Important: if there are missing vars in the templateDictionary, an error will be trown\n",
    "for f in os.listdir(preRenderedDir):\n",
    "\tif not f.startswith('.'):\n",
    "\t\twith open(f\"{preRenderedDir}/{f}\", \"r\") as ft:\n",
    "\t\t\ttemplate = TraceTemplate(ft.read())\n",
    "\t\t\trenderedTemplate = template.substitute(templateDictionary)\n",
    "\t\twith open(f\"{renderedDir}/{f}\", \"w\") as fr:\n",
    "\t\t\tfr.write(renderedTemplate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise-venv",
   "language": "python",
   "name": "exercise-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
