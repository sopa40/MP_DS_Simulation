import argparse
import collections
import logging
import re

import common


def load_files(args):
    input_files_by_task = collections.defaultdict(lambda: collections.defaultdict())
    output_files_by_task = collections.defaultdict(lambda: collections.defaultdict())
    working_dirs = common.load_working_dirs(args.workdirfile)

    for hash_id, v in working_dirs.items():
        working_dir = v['workdir'].replace(args.nextflow, args.local)
        with open("%s/.inputs.trace" % working_dir, "r") as file:
            for line in file:
                path, size = line.split(":")
                input_files_by_task[hash_id][path] = int(size.strip())

        with open("%s/.outputs.trace" % working_dir, "r") as file:
            for line in file:
                path, size = line.split(":")
                output_files_by_task[hash_id][path] = int(size.strip())

    return input_files_by_task, output_files_by_task


def fix_tmp_dir_outputs(filtered_output, working_dirs):
    pattern = re.compile("/tmp/nxf\.[0-9A-Za-z]{10}")
    fixed_output_dirs = {}

    for hash_id, v in working_dirs.items():
        outputs_for_task = filtered_output[hash_id]
        fixed_output_dirs_for_task = {}
        workdir = v['workdir']

        for path, size in outputs_for_task.items():
            old = path
            path = pattern.sub(workdir, path)
            logging.debug("%s -> %s", old, path)
            fixed_output_dirs_for_task[path] = size

        fixed_output_dirs[hash_id] = fixed_output_dirs_for_task

    return fixed_output_dirs


def filter_input_files(args):
    working_dirs = common.load_working_dirs(args.workdirfile)
    input_files, output_files = load_files(args)

    number_of_tasks = len(working_dirs)
    logging.info("Filtering Inputs/Outputs of %d Tasks based on strace reports", number_of_tasks)

    i = 0

    for hash_id, v in working_dirs.items():
        if i % 10 == 0:
            logging.info("Progress: %d%%", int((i / number_of_tasks) * 100))

        i += 1
        inputs_for_task = input_files[hash_id]
        outputs_for_task = output_files[hash_id]
        distinct_reads = set()
        distinct_write = set()
        # F-Up on my side, sometimes the folder is called workdir sometimes work
        for pid, strace in common.load_strace_files_from_working_dir(v['workdir'].replace(args.nextflow, args.local)):
            matches_reads = re.findall(r"\d{2}:\d{2}:\d{2}.\d{6} read\(\d<(.*)>,.* \d*\) = \d* <\d*\.\d*>", strace)
            matches_write = re.findall(r"\d{2}:\d{2}:\d{2}.\d{6} write\(\d<(.*)>,.* \d*\) = \d* <\d*\.\d*>", strace)

            matches_reads = list(filter(lambda t: t in inputs_for_task.keys(), matches_reads))
            matches_write = list(filter(lambda t: t in outputs_for_task.keys(), matches_write))

            for strace_filename in matches_reads:
                distinct_reads.add(strace_filename)
            for strace_filename in matches_write:
                distinct_write.add(strace_filename)
        n_unfiltered_inputs, n_unfiltered_outputs = len(inputs_for_task), len(outputs_for_task)

        for k in set(inputs_for_task) - distinct_reads:
            del inputs_for_task[k]

        for k in set(outputs_for_task) - distinct_write:
            del outputs_for_task[k]

        logging.info("Inputs: %d -> %d | Outputs: %d -> %d",
                     n_unfiltered_inputs,
                     len(inputs_for_task),
                     n_unfiltered_outputs,
                     len(outputs_for_task))

    return input_files, fix_tmp_dir_outputs(output_files, working_dirs)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Exclude unused files from the IOFile generated by Nextflow using the strace output')
    parser.add_argument('workdirfile', type=str,
                        help='path to the workdirs file with the format <hash> <workdir> <name>. Can be generated using nextflow log')
    parser.add_argument('--nextflow', dest='nextflow', default='/workdir', type=str,
                        help='the working directory path that was used during the nextflow execution, and is thus used inside the iofile and workdir file')
    parser.add_argument('--analyzer', dest='local', default='../work', type=str,
                        help='path to the working directory on the local machine')
    parser.add_argument('--output', dest='output', default='filtered.txt', type=str,
                        help='path to the output file')
    parser.add_argument('-v', dest='logging', default='INFO', type=str,
                        help='logging level')

    args = parser.parse_args()

    logging.basicConfig()
    logging.root.setLevel(args.logging)
    filterd_input, filtered_output = filter_input_files(args)
    common.write_in_out_file(filterd_input, filtered_output, args.output)
